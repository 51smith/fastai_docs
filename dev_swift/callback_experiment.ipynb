{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebooks\")' FastaiNotebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Callback Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TensorFlow\n",
    "\n",
    "// Please pick a better name for me! :-)\n",
    "enum CallbackException {\n",
    "    case cancelTraining\n",
    "    case cancelEpoch\n",
    "    case cancelBatch\n",
    "}\n",
    "\n",
    "enum CallbackEvent {\n",
    "    // I haven't implemented all the events.\n",
    "    case beginFit\n",
    "    case beginEpoch\n",
    "    case beginBatch\n",
    "    case afterForwardsBackwards\n",
    "    case afterFit\n",
    "}\n",
    "\n",
    "func defaultCallback(e: CallbackEvent) {}\n",
    "\n",
    "struct DataBatch<Inputs: Differentiable & TensorGroup, Labels: TensorGroup>: TensorGroup {\n",
    "    var xb: Inputs\n",
    "    var yb: Labels    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner<Opt: Optimizer, Labels: TensorGroup>\n",
    "where Opt.Model.CotangentVector == Opt.Model.AllDifferentiableVariables,\n",
    "      Opt.Model.Input: TensorGroup\n",
    "{\n",
    "    typealias Model = Opt.Model\n",
    "    var model: Model\n",
    "    \n",
    "    typealias Inputs = Model.Input\n",
    "    \n",
    "    // I'm getting some crashes in AD-generated code if I put a `lossFunc` in the learner.\n",
    "    // So I'm putting a `lossWithGradient` for now, to work around this.\n",
    "    // (model, context, inputs, labels) -> (loss, grad)\n",
    "    typealias LossWithGradient = (Model, Context, Inputs, Labels) -> (Tensor<Float>, Model.AllDifferentiableVariables)\n",
    "    var lossWithGradient: LossWithGradient\n",
    "    \n",
    "    var optimizer: Opt\n",
    "    \n",
    "    typealias Data = Dataset<DataBatch<Inputs, Labels>>\n",
    "    var data: Data\n",
    "    \n",
    "    var context: Context = Context(learningPhase: .training)\n",
    "\n",
    "    typealias Callback = (CallbackEvent) throws -> ()    \n",
    "    var callback: Callback = defaultCallback\n",
    "    \n",
    "    var loss: Tensor<Float> = Tensor(0)\n",
    "    var grad: Model.AllDifferentiableVariables = Model.AllDifferentiableVariables.zero\n",
    "    \n",
    "    var epoch: Int = 0\n",
    "    var epochs: Int = 0\n",
    "    \n",
    "    init(\n",
    "        model: Model,\n",
    "        lossWithGradient: @escaping LossWithGradient,\n",
    "        optimizer: Opt,\n",
    "        data: Data\n",
    "    ) {\n",
    "        self.model = model\n",
    "        self.lossWithGradient = lossWithGradient\n",
    "        self.optimizer = optimizer\n",
    "        self.data = data\n",
    "    }\n",
    "    \n",
    "    func trainOneBatch(xb: Inputs, yb: Labels) throws {\n",
    "        try callback(.beginBatch)\n",
    "        (self.loss, self.grad) = lossWithGradient(model, self.context, xb, yb)\n",
    "        defer {\n",
    "            // Zero out the loss & gradient to ensure stale values aren't used.\n",
    "            self.loss = Tensor(0)\n",
    "            self.grad = Model.AllDifferentiableVariables.zero        \n",
    "        }\n",
    "        try callback(.afterForwardsBackwards)\n",
    "        optimizer.update(&model.allDifferentiableVariables, along: self.grad)\n",
    "    }\n",
    "    \n",
    "    func trainOneEpoch() throws {\n",
    "        try callback(.beginEpoch)\n",
    "        for batch in self.data {\n",
    "            do {\n",
    "                try trainOneBatch(xb: batch.xb, yb: batch.yb)\n",
    "            } catch CallbackException.cancelBatch {}  // Continue\n",
    "        }\n",
    "    }\n",
    "\n",
    "    func fit(epochs: Int) throws {\n",
    "        // I haven't implemented validation.\n",
    "        self.epochs = epochs\n",
    "        do {\n",
    "            try callback(.beginFit)\n",
    "            defer {\n",
    "                do {\n",
    "                    try callback(.afterFit)\n",
    "                } catch {\n",
    "                    print(\"Error during callback(.afterFit): \\(error)\")\n",
    "                }\n",
    "            }\n",
    "            for epoch in 1...epochs {\n",
    "                self.epoch = epoch\n",
    "                do {\n",
    "                    try trainOneEpoch()\n",
    "                } catch let error as CallbackException where error != .cancelTraining {}  // Continue\n",
    "            }\n",
    "        } catch is CallbackException {}  // Catch all CallbackExceptions.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement some example callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func installProgress1<Opt, Labels>(on learner: Learner<Opt, Labels>) {\n",
    "    let chainedCallback = learner.callback  // Keep a handle to the current callback.\n",
    "    learner.callback = { event in\n",
    "        switch event {\n",
    "        case .beginEpoch:\n",
    "            print(\"Starting new epoch: \\(learner.epoch) of \\(learner.epochs)!\")\n",
    "        default: break\n",
    "        }\n",
    "        try chainedCallback(event)  // Don't forget to call the previous callback!\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Helper so you don't need to do the chaining yourself. :-)\n",
    "func chainCallback<Opt, Labels>(on learner: Learner<Opt, Labels>, newCallback: @escaping (CallbackEvent) throws -> ()) {\n",
    "    let existingCallback = learner.callback\n",
    "    learner.callback = { event in\n",
    "        try newCallback(event)\n",
    "        try existingCallback(event)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func installProgress<Opt, Labels>(on learner: Learner<Opt, Labels>) {\n",
    "    chainCallback(on: learner) { event in\n",
    "        switch event {\n",
    "        case .beginEpoch:\n",
    "            print(\"Starting new epoch: \\(learner.epoch) of \\(learner.epochs)!\")\n",
    "        default: break\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "let plt = Python.import(\"matplotlib.pyplot\")\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")\n",
    "\n",
    "public class RecordedInfo {\n",
    "    public var losses: [Float] = []\n",
    "    public var lrs: [Float] = []\n",
    "    \n",
    "    func plot() {\n",
    "        plt.plot(self.losses)\n",
    "        plt.plot(self.lrs)\n",
    "        // print(losses)\n",
    "        // print(lrs)\n",
    "    }\n",
    "}\n",
    "\n",
    "func installRecorder<Opt, Labels>(on learner: Learner<Opt, Labels>) -> RecordedInfo where Opt.Scalar == Float {\n",
    "    let recorder = RecordedInfo()\n",
    "    chainCallback(on: learner) { event in \n",
    "        switch event {\n",
    "        case .beginFit:\n",
    "            recorder.losses = []\n",
    "            recorder.lrs = []\n",
    "        case .afterForwardsBackwards:\n",
    "            recorder.losses.append(learner.loss.scalar!)\n",
    "            recorder.lrs.append(learner.optimizer.learningRate)\n",
    "        default: break\n",
    "        }\n",
    "    }\n",
    "    return recorder\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func installParameterScheduler<Opt, Labels, Param>(\n",
    "    on learner: Learner<Opt, Labels>,\n",
    "    forParameter paramKeyPath: ReferenceWritableKeyPath<Learner<Opt, Labels>, Param>,\n",
    "    schedule: @escaping (Float) -> Param) {\n",
    "    chainCallback(on: learner) { event in\n",
    "        switch event {\n",
    "        case .beginBatch:\n",
    "            learner[keyPath: paramKeyPath] = schedule(Float(learner.epoch) / Float(learner.epochs))\n",
    "        default: break;\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebooks\n",
    "import Path\n",
    "\n",
    "var (xTrain,yTrain,xValid,yValid) = loadMNIST(path: Path.home/\".fastai\"/\"data\"/\"mnist_tst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = xTrain.reshaped(toShape: [60000, 784])\n",
    "\n",
    "let (n,m) = (Int(xTrain.shape[0]),Int(xTrain.shape[1]))\n",
    "let c = yTrain.max()+1\n",
    "\n",
    "let nh = 50\n",
    "let bs: Int32 = 64\n",
    "\n",
    "let train_ds: Dataset<DataBatch> = Dataset(elements: DataBatch(xb: xTrain, yb: yTrain)).batched(Int64(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let outputCount = 10\n",
    "\n",
    "struct MyModel: Layer {\n",
    "    var layer1 = Dense<Float>(inputSize: m, outputSize: nh, activation: relu)\n",
    "    var layer2 = Dense<Float>(inputSize: nh, outputSize: outputCount)\n",
    "    \n",
    "    /// A silly non-trained parameter to show off the parameter scheduler.\n",
    "    @noDerivative var sillyExtraBiasParam: Tensor<Float> = Tensor(zeros: [Int32(outputCount)])\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        return input.sequenced(in: context, through: layer1, layer2) + sillyExtraBiasParam\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = MyModel()\n",
    "\n",
    "func lossWithGrad(\n",
    "    model: MyModel,\n",
    "    in context: Context,\n",
    "    inputs: Tensor<Float>,\n",
    "    labels: Tensor<Int32>\n",
    ") -> (Tensor<Float>, MyModel.AllDifferentiableVariables) {\n",
    "    return model.valueWithGradient { model -> Tensor<Float> in\n",
    "        let predictions = model.applied(to: inputs, in: context)\n",
    "        return softmaxCrossEntropy(logits: predictions, labels: labels)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Some typealiases to reduce repeatedly typing types.\n",
    "typealias MyOptimizer = SGD<MyModel, Float>\n",
    "typealias MyLearner = Learner<MyOptimizer, Tensor<Int32>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let optimizer = MyOptimizer(learningRate: 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(\n",
    "    model: model,\n",
    "    lossWithGradient: lossWithGrad,\n",
    "    optimizer: optimizer,\n",
    "    data: train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We can't schedule the learning rate because the Optimizer protocol doesn't allow setting learning rates.\n",
    "// If we change it to allow setting learning rates, `ParamScheduler` should allow setting learning rates,\n",
    "// with `paramKeyPath: \\MyLearner.optimizer.learningRate`.\n",
    "installParameterScheduler(on: learner, forParameter: \\MyLearner.model.sillyExtraBiasParam) { t in\n",
    "    if t < 0.5 {\n",
    "        return Tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    } else {\n",
    "        return Tensor([10, 20, 30, 0, 0, 0, 0, 0, 0, 0])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let recorder = installRecorder(on: learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installProgress(on: learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(epochs: 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
