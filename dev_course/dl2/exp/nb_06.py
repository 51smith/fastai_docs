
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/06_cuda_cnn_bn.ipynb

from exp.nb_05 import *
torch.set_num_threads(2)

def normalize_to(train, valid):
    m,s = train.mean(),train.std()
    return normalize(train, m, s), normalize(valid, m, s)

class Lambda(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, x): return self.func(x)

def flatten(x):      return x.view(x.shape[0], -1)

def get_runner(model, lr=0.6, cbs=None, loss_func = F.cross_entropy):
    opt = optim.SGD(model.parameters(), lr=lr)
    learn = Learner(model, opt, loss_func, data)
    return learn, Runner([AvgStatsCallback([accuracy])] + listify(cbs))

class CudaCallback(Callback):
    def begin_fit(self, run): run.model.cuda()
    def begin_batch(self, run): run.xb,run.yb = run.xb.cuda(),run.yb.cuda()

class BatchTransformXCallback(Callback):
    _order=2
    def __init__(self, tfm): self.tfm = tfm
    def begin_batch(self, run): run.xb = self.tfm(run.xb)

def resize_tfm(*size):
    def _inner(x): return x.view(*((-1,)+size))
    return _inner

def get_cnn_model(data, nfs):
    nfs = [1] + nfs + [data.c]
    layers = [
        conv2d(nfs[i], nfs[i+1], 5 if i==1 else 3, act=i!=len(nfs)-1)
        for i in range(len(nfs)-1)
    ]
    return nn.Sequential(
        *layers,
        nn.AdaptiveAvgPool2d(1),
        Lambda(flatten)
    )

from torch.jit import ScriptModule, script_method, script
from typing import *

def conv2d(ni, nf, ks=3, stride=2, act=True, bn=True):
    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn)]
    if act: layers.append(nn.ReLU())
    if bn: layers.append(nn.BatchNorm2d(nf))
    return nn.Sequential(*layers)