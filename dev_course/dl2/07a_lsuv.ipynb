{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the MNIST data and a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "\n",
    "x_train,x_valid = normalize_to(x_train,x_valid)\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "\n",
    "nh,bs = 50,512\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_view = view_tfm(1,28,28)\n",
    "cbfs = [Recorder,\n",
    "        partial(AvgStatsCallback,accuracy),\n",
    "        CudaCallback,\n",
    "        partial(BatchTransformXCallback, mnist_view)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [8,16,32,64,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(ni, nf, ks=3, stride=2, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True),\n",
    "        GeneralRelu(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the [original paper](https://arxiv.org/pdf/1511.06422.pdf). We initialize our neural net with the usual technique, then we pass a batch through the model and check the outputs of the linear and convolutional layers. We can then rescale the weights according to the variance we observe on the activations, and subtract the mean we observe from the initial bias. That way we will have activations that stay normalize.\n",
    "\n",
    "We repeat this process until we are satisfied with the mean/variance we observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dl, run):\n",
    "    run.xb,run.yb = next(iter(dl))\n",
    "    for cb in run.cbs: cb.set_runner(run)\n",
    "    run('begin_batch')\n",
    "    return run.xb,run.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = get_batch(data.train_dl, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the outputs of convolutional or linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_modules(m, cond):\n",
    "    if cond(m): return [m]\n",
    "    return sum([find_modules(o,cond) for o in m.children()], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lin_layer(l):\n",
    "    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear)\n",
    "    return isinstance(l, lin_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods = find_modules(learn.model, is_lin_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_mean(hook, mod, inp, outp):\n",
    "    d = outp.data\n",
    "    hook.mean,hook.std = d.mean().item(),d.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = learn.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008279498666524887 1.4534205198287964\n",
      "-0.18544313311576843 1.3938395977020264\n",
      "-0.03607095032930374 1.150250792503357\n",
      "-0.035806965082883835 1.057843804359436\n",
      "-0.04104777052998543 0.7038090229034424\n",
      "-0.07098027318716049 0.26428452134132385\n"
     ]
    }
   ],
   "source": [
    "with Hooks(mods,append_mean) as hooks:\n",
    "    mdl(xb)\n",
    "    for hook in hooks: print(hook.mean,hook.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is then to change the bias and weights accordingly to make the activations have a mean closer to 0 and a std closer to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsuv_module(m, xb):\n",
    "    h = Hook(m, append_mean)\n",
    "\n",
    "    if getattr(m, 'bias', None) is not None:\n",
    "        while mdl(xb) is not None and abs(h.mean) > 1e-3:\n",
    "            m.bias.data -= h.mean\n",
    "\n",
    "    while mdl(xb) is not None and abs(h.std-1) > 1e-3:\n",
    "        m.weight.data /= h.std\n",
    "\n",
    "    h.remove()\n",
    "    return h.mean,h.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.002582934917882085, 0.9999999403953552)\n",
      "(-0.0059326705522835255, 1.0)\n",
      "(-0.004340727347880602, 1.0000001192092896)\n",
      "(-0.002693064510822296, 0.9999999403953552)\n",
      "(-0.021042972803115845, 1.0)\n",
      "(-0.22801573574543, 0.9995047450065613)\n"
     ]
    }
   ],
   "source": [
    "for m in mods: print(lsuv_module(m, xb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then training is beginning on better grounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.04834015625, tensor(0.6594, device='cuda:0')]\n",
      "valid: [0.1922559326171875, tensor(0.9396, device='cuda:0')]\n",
      "train: [0.17182677734375, tensor(0.9472, device='cuda:0')]\n",
      "valid: [0.1034960693359375, tensor(0.9676, device='cuda:0')]\n",
      "CPU times: user 2.38 s, sys: 519 ms, total: 2.9 s\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%time run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
