{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": "from fastai.gen_doc.gen_notebooks import update_nb_metadata\n# For updating jekyll metadata. You MUST reload notebook immediately after executing this cell for changes to save\nupdate_nb_metadata('callbacks.ipynb', title=None, summary=None)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# List of callbacks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": "from fastai.gen_doc.nbdoc import *\nfrom fastai.callbacks import * \nfrom fastai.basic_train import * \nfrom fastai.train import * \nfrom fastai import callbacks"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "fastai's training loop is highly extensible, with a rich *callback* system. See the `callback` docs if you're interested in writing your own callback. See below for a list of callbacks that are provided with fastai, grouped by the module they're defined in.\n\nEvery callback that is passed to `Learner` with the `callback_fns` parameter will be automatically stored as an attribute. The attribute name is snake-cased, so for instance `ActivationStats` will appear as `learn.activation_stats` (assuming your object is named `learn`)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## `callbacks`\n\nThis sub-package contains more sophisticated callbacks that each are in their own module. They are (click the link for more details):\n\n### `OneCycleScheduler`\n\nTrain with Leslie Smith's [1cycle annealing](https://sgugger.github.io/the-1cycle-policy.html) method.\n\n### `MixedPrecision`\n\nUse fp16 to [take advantage of tensor cores](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html) on recent NVIDIA GPUs for a 200% or more speedup.\n\n### `GeneralScheduler`\n\nCreate your own multi-stage annealing schemes with a convenient API.\n\n### `MixUpCallback`\n\nData augmentation using the method from [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)\n\n### `LRFinder`\n\nUse Leslie Smith's [learning rate finder](https://www.jeremyjordan.me/nn-learning-rate/) to find a good learning rate for training your model.\n\n### `HookCallback`\n\nConvenient wrapper for registering and automatically deregistering [PyTorch hooks](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#forward-and-backward-function-hooks). Also contains pre-defined hook callback: `ActivationStats`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## `train` and `basic_train`\n\n### `Recorder`\n\nTrack per-batch and per-epoch smoothed losses and metrics.\n\n### `ShowGraph`\n\nDynamically display a learning chart during training.\n\n### `BnFreeze`\n\nFreeze batchnorm layer moving average statistics for non-trainable layers."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
