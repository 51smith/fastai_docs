{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train` provides a number of extension methods that are added to `Learner` (see below for a list and details), along with three simple callbacks:\n",
    "\n",
    "- `ShowGraph`\n",
    "- `GradientClipping`\n",
    "- `BnFreeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from fastai.gen_doc.nbdoc import *\n",
    "from fastai.train import *\n",
    "from fastai.docs import *\n",
    "from fastai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner` extension methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are automatically added to all `Learner` objects created after importing this module. They provide convenient access to a number of callbacks, without requiring them to be manually created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=fit_one_cycle></a>`fit_one_cycle`\n",
       "> `fit_one_cycle`(`learn`:`Learner`, `cyc_len`:`int`, `max_lr`:`Union`\\[`float`, `Collection`\\[`float`\\], `slice`\\]=`slice(None, 0.003, None)`, `moms`:`Point`=`(0.95, 0.85)`, `div_factor`:`float`=`25.0`, `pct_start`:`float`=`0.3`, `wd`:`float`=`None`, `kwargs`)\n",
       "\n",
       "\n",
       "Fit a model following the 1cycle policy. <a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L11\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(fit_one_cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a model with 1cycle training. See `OneCycleScheduler` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=lr_find></a>`lr_find`\n",
       "> `lr_find`(`learn`:`Learner`, `start_lr`:`float`=`1e-05`, `end_lr`:`float`=`10`, `num_it`:`int`=`100`, `kwargs`:`Any`)\n",
       "\n",
       "\n",
       "Explore lr from `start_lr` to `end_lr` over `num_it` iterations in `learn`. <a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L20\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(lr_find)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `LRFinder` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=to_fp16></a>`to_fp16`\n",
       "> `to_fp16`(`learn`:`Learner`, `loss_scale`:`float`=`512.0`, `flat_master`:`bool`=`False`) -> `Learner`\n",
       "\n",
       "\n",
       "Transform `learn` in FP16 precision. <a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L26\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(to_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `MixedPrecision` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=mixup></a>`mixup`\n",
       "> `mixup`(`learn`:`Learner`, `alpha`:`float`=`0.4`, `stack_x`:`bool`=`False`, `stack_y`:`bool`=`True`) -> `Learner`\n",
       "\n",
       "\n",
       "Add mixup https://arxiv.org/abs/1710.09412 to `learn`. <a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L33\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(mixup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `MixUpCallback` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last extension method comes from the module tta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=TTA></a>`TTA`\n",
       "> `TTA`(`learn`:`Learner`, `beta`:`float`=`0.4`, `scale`:`float`=`1.35`, `is_test`:`bool`=`False`) -> `Tensors`\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/tta.py#L45\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.TTA, full_name='TTA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies Test Time Augmentation to `learn` on the validation set or the test test (depending on `is_test`). We take the average of our regular predictions (with a weight `beta`) with the average of predictions obtained thourh augmented versions of the training set (with a weight `1-beta`). The transforms decided for the training set are applied with a few changes `scale` controls the scale for zoom (which isn't random), the cropping isn't random but we make sure to get the four corners of the image. Flipping isn't random but applied once on each of those corner images (so that makes 8 augmented versions total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=ShowGraph></a>`class` `ShowGraph`\n",
       "> `ShowGraph`(`learn`:`Learner`) :: `LearnerCallback`\n",
       "\n",
       "\n",
       "Update a graph of learner stats and metrics after each epoch. <a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L44\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ShowGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy, callback_fns=ShowGraph)\n",
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training graph](imgs/train_graph.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_epoch_end></a>`on_epoch_end`\n",
       "> `on_epoch_end`(`n_epochs`:`int`, `last_metrics`:`MetricsList`, `kwargs`) -> `bool`\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L46\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ShowGraph.on_epoch_end, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have `last_metrics`, plot them in `self.pbar`. Set the size of the graph with `n_epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=GradientClipping></a>`class` `GradientClipping`\n",
       "> `GradientClipping`(`learn`:`Learner`, `clip`:`float`) :: `LearnerCallback`\n",
       "\n",
       "\n",
       "To do gradient clipping during training. <a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L64\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GradientClipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clips gradient at a maximum absolute value of `clip` during training. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value=''))), HTML(value='epoch  train loss  vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:05\n",
      "epoch  train loss  valid loss  accuracy\n",
      "0      0.080691    0.043826    0.986261  (00:05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy,\n",
    "    callback_fns=partial(GradientClipping, clip=0.1))\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_backward_end></a>`on_backward_end`\n",
       "> `on_backward_end`(`kwargs`)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L68\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GradientClipping.on_backward_end, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the gradients after they are computed but before the optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=BnFreeze></a>class `BnFreeze`\n",
       "`BnFreeze`(`learn`:`Learner`) :: `LearnerCallback`\n",
       "\n",
       "\n",
       "Freezes moving average statistics in all batchnorm layers <a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L57\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BnFreeze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For batchnorm layers where `requires_grad==False`, you generally don't want to update their moving average statistics, in order to avoid the model's statistics getting out of sync with its pre-trained weights. You can add this callback to automate this freezing of statistics (internally, it calls `eval` on these layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value=''))), HTML(value='epoch  train loss  vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:05\n",
      "epoch  train loss  valid loss  accuracy\n",
      "0      0.085482    0.052628    0.984789  (00:05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy, callback_fns=BnFreeze)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_epoch_begin></a>`on_epoch_begin`\n",
       "> `on_epoch_begin`(`kwargs`:`Any`)\n",
       "<a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L59\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BnFreeze.on_epoch_begin, doc_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set back the batchnorm layers on `eval` mode after the model has been set to `train`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undocumented Methods - Methods moved below this line will intentionally be hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "show_doc(one_cycle_scheduler)"
   ]
  }
 ],
 "metadata": {
  "jekyll": {
   "summary": "Extensions to Learner that easily implement Callback",
   "title": "train"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
