{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.docs import *\n",
    "from fastai.text import *\n",
    "torch.backends.cudnn.benchmark=True\n",
    "import json\n",
    "\n",
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeVise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we will implement the [DeVise paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf). What makes this paper specially interesting is that it combines image classification and text embeddings. The technique presented by the authors leverages word embeddings to assign several possible tags to each image. By doing this, the model fares considerably well (achieving up to 18% hit rates) in never seen before categories (zero-shot learning). But how can the model classify objects it has never seen before? That is the power of word embeddings.\n",
    "\n",
    "Basically, the model will use the 'closeness' of several words it knows through the embeddings to classify a new image. Perhaps this is easiest explained through a human example. When we are teaching a toddler what a motorcycle is, for example, we might say \"Well, it is a bicycle but it goes faster\". That is we relate it to what *he/she already knows*. In the same way, if the model sees a trout, it might say \"Well, I know it is very similar to a trench and I know what a trench is so I will say it is either a trench or something very similar, like a sea bass or a trout\". In 2D these relationships would look like this:\n",
    "\n",
    "![clusters](imgs/clusters.png)\n",
    "Frome et al., 2013\n",
    "\n",
    "Please consider that while you may say \"Obviously, a goldfish has to do more with a shark than with an iguana because they are both aquatic\" you are comparing these across one dimension, namely natural habitat, while if you compared them by size the results would be different. These infinite dimensions across which you can compare two words are resumed into a finite number of categories which is what we call embeddings. In this image, we are arbitrarily choosing one dimension to make the point since it is intuitive to us human beings.\n",
    "\n",
    "To create this network the authors combined a computer vision architecture with the embeddings data to create a hybrid model that we can see in the following picture:\n",
    "\n",
    "![devise_arch](imgs/devise_arch.png)\n",
    "Frome et al., 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/imagenet/test')\n",
    "TMP_PATH = Path('../data/imagenet/tmp')\n",
    "TRANS_PATH = Path('../data/translate/')\n",
    "PATH_TRN = PATH/'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to load our word vectors. We'll see that each word has a normalized number between [-1, 1] for each of the 300 embeddings. This is effectively a 300 dimension representation of the meaning of each word. As an example, let's see the embedding for 'king'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-3ab9b4c448c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mft_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRANS_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'wiki.en.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastText/FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;34m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastText/FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_vecs = ft.load_model(str((TRANS_PATH/'wiki.en.bin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vecs.get_word_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how correlated two words are in how close these numbers are for each embedding. For example we would stipulate that 'jeremy' and 'Jeremy' are more related than 'banana' and 'Jeremy'. Let's see if our embeddings think alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.60866078],\n",
       "       [0.60866078, 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ft_vecs.get_word_vector('jeremy'), ft_vecs.get_word_vector('Jeremy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.14482342],\n",
       "       [0.14482342, 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ft_vecs.get_word_vector('banana'), ft_vecs.get_word_vector('Jeremy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map imagenet classes to word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get all the words in the dictionary and sort them by their frequency (how often do they appear in the aforementioned datasets). We will then count how many words do we have in our dictionary as another step in the 'discovery phase' of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = ft_vecs.get_words(include_freq=True)\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the names of our 1000 imagenet classes so that we can assign each class in our imagenet dataset to a 300-long embedding (for that we need the actual word-id for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_FN = 'imagenet_class_index.json'\n",
    "download_url(f'http://files.fast.ai/models/{CLASSES_FN}', TMP_PATH/CLASSES_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download all the nouns in English from WORDNET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_FN = 'classids.txt'\n",
    "download_url(f'http://files.fast.ai/data/{WORDS_FN}', PATH/WORDS_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will build a dictionary that maps our classes to the word-id for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = json.load((TMP_PATH/CLASSES_FN).open())\n",
    "classids_1k = dict(class_dict.values())\n",
    "nclass = len(class_dict); nclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that our class-id assignments are made correctly. Here we can see our two worlds:\n",
    "\n",
    "1. Imagenet and its class to id mapping\n",
    "2. WORDNET and its class to id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n00001740 entity\\n',\n",
       " 'n00001930 physical_entity\\n',\n",
       " 'n00002137 abstraction\\n',\n",
       " 'n00002452 thing\\n',\n",
       " 'n00002684 object\\n']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classid_lines = (PATH/WORDS_FN).open().readlines()\n",
    "classid_lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the nouns in the English language and the Imagenet class ids, we need to connect each of these with the words in fastText. We will do this by creating a dictionary of synset to word vectors for both our WORDNET and Imagenet lists that __will only keep the words that are present in both datasets__ (i.e. both WORDNET and fastText for *syn_wv* and both Imagenet and fastText for *syn_wv_1k*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classids = dict(l.strip().split() for l in classid_lines)\n",
    "len(classids),len(classids_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_vec_d = {w.lower(): ft_vecs.get_word_vector(w) for w in ft_words[-1000000:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fe7ccf6ebc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()\n\u001b[0m\u001b[1;32m      2\u001b[0m           if v.lower() in lc_vec_d]\n\u001b[1;32m      3\u001b[0m syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()\n\u001b[1;32m      4\u001b[0m           if v.lower() in lc_vec_d]\n\u001b[1;32m      5\u001b[0m \u001b[0msyn2wv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_wv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classids' is not defined"
     ]
    }
   ],
   "source": [
    "syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()\n",
    "          if v.lower() in lc_vec_d]\n",
    "syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()\n",
    "          if v.lower() in lc_vec_d]\n",
    "syn2wv = dict(syn_wv)\n",
    "len(syn2wv), len(syn_wv_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'syn2wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-46ea570c57a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn2wv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTMP_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'syn2wv.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_wv_1k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTMP_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'syn_wv_1k.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'syn2wv' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(syn2wv, (TMP_PATH/'syn2wv.pkl').open('wb'))\n",
    "pickle.dump(syn_wv_1k, (TMP_PATH/'syn_wv_1k.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn2wv = pickle.load((TMP_PATH/'syn2wv.pkl').open('rb'))\n",
    "syn_wv_1k = pickle.load((TMP_PATH/'syn_wv_1k.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is building the data we are going to train our model on. For that we are only including images with ids that are English nouns. Our _x_ variables will be our images (which we are saving in a PosixPath format) and our *y* variables will be our vectors (300 floats, one for each embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "img_vecs = []\n",
    "images_val = []\n",
    "img_vecs_val = []\n",
    "\n",
    "for d in (PATH/'train').iterdir():\n",
    "    if d.name not in syn2wv: continue\n",
    "    vec = syn2wv[d.name]\n",
    "    for f in d.iterdir():\n",
    "        images.append(str(f.relative_to(PATH)))\n",
    "        img_vecs.append(vec)\n",
    "\n",
    "n_val=0\n",
    "for d in (PATH/'valid').iterdir():\n",
    "    if d.name not in syn2wv: continue\n",
    "    vec = syn2wv[d.name]\n",
    "    for f in d.iterdir():\n",
    "        images_val.append(str(f.relative_to(PATH)))\n",
    "        img_vecs_val.append(vec)\n",
    "        n_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5300, 300)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vecs = np.stack(img_vecs)\n",
    "img_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(images, (TMP_PATH/'images.pkl').open('wb'))\n",
    "pickle.dump(img_vecs, (TMP_PATH/'img_vecs.pkl').open('wb'))\n",
    "pickle.dump(images_val, (TMP_PATH/'images_val.pkl').open('wb'))\n",
    "pickle.dump(img_vecs_val, (TMP_PATH/'img_vecs)val.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pickle.load((TMP_PATH/'images.pkl').open('rb'))\n",
    "img_vecs = pickle.load((TMP_PATH/'img_vecs.pkl').open('rb'))\n",
    "images_val = pickle.load((TMP_PATH/'images_val.pkl').open('rb'))\n",
    "img_vecs_val = pickle.load((TMP_PATH/'img_vecs_val.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our dataset and create our DataBunch object. Note that we will need to tell our model how many classes we have. We will specify this manually since our ImageDataset class does not support it natively (this argument will then be passed to our model). We will resize our pictures to a 224x224 size and normalize them. Finally we will check that our data looks as we would like it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/francisco/fastai/fastai/basic_train.py\", line 160, in __del__\n",
      "    def __del__(self): del(self.model, self.data)\n",
      "AttributeError: model\n"
     ]
    }
   ],
   "source": [
    "folder_path = (PATH/\"\").absolute()\n",
    "images = [folder_path/image for image in images]\n",
    "images_val = [folder_path/image_val for image_val in images_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5300"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_val[0]\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageDataset(images, img_vecs)\n",
    "valid_ds = ImageDataset(images_val, img_vecs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.classes = range(300)\n",
    "valid_ds.classes = range(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = ([flip_lr()], [crop_pad(size=224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, path=PATH, device=torch.device('cuda'), ds_tfms = get_transforms(), tfms=imagenet_norm, size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(data.valid_dl))\n",
    "x.size(),y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train our model. Our model will try to predict the value of each embedding for each of our images. To accomplish this we will add a fully connected layer at the end of our resnet50 architecture (with 300 output neurons) and precompute the activations of the backbone model so as to save training time. We will also initialize the weights of the backbone model with the weights of the pretrained model. Given that the pretrained model and ours are both training in the same dataset we will not need to do any finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to start by precomputing our activations for the convolutional backbone and we will then train our head from these activations (so our model will not have to calculate them again for each epoch). Since we already have a pretrained backbone, we will just need to run one forward pass to compute the final activations; that is we will not need any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz, threading\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(data, tvm.resnet50, ps=[0.2,0.2], pretrained=True, callback_fns=BnFreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = learn.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(body.children())\n",
    "layers += [AdaptiveConcatPool2d(), Flatten()]   \n",
    "body = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = num_features(body)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt_fn = partial(AdamW, betas=(0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_loss(inp,targ): return 1 - F.cosine_similarity(inp,targ).mean()\n",
    "learn.loss_fn = cos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(path, model_name, tmp_path, nf, force=False): \n",
    "    tmpl = f'_{model_name}.bc'\n",
    "    names = [os.path.join(path/'tmp', p+tmpl) for p in ('x_act', 'x_act_val')]\n",
    "    if os.path.exists(names[0]) and not force:\n",
    "        activations = [bcolz.open(p) for p in names]\n",
    "    else:\n",
    "        activations = [create_empty_bcolz(nf,p) for p in names]\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_bcolz(n, name):\n",
    "    return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_bcolz(m, gen, arr, workers=4):\n",
    "    arr.trim(len(arr))\n",
    "    lock=threading.Lock()\n",
    "    m.eval()\n",
    "    for x,*_ in tqdm(gen):\n",
    "        y = to_np((m(x.data)).detach())\n",
    "        y = np.moveaxis(y,1,-1)\n",
    "        with lock:\n",
    "            arr.append(y)\n",
    "    arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fc1(data, model, path, model_name, tmp_path, nf):\n",
    "    activations = get_activations(path, model_name, tmp_path, nf)\n",
    "    act=activations[0]\n",
    "    val_act=activations[1]\n",
    "    m=model\n",
    "    if len(act)!=len(data.train_ds):\n",
    "        act = predict_to_bcolz(m, data.train_dl, act)\n",
    "    if len(val_act)!=len(data.valid_ds):\n",
    "        val_act = predict_to_bcolz(m, data.valid_dl, val_act)\n",
    "    \n",
    "    fc_data = FCDataset(act, img_vecs)\n",
    "    fc_data_val = FCDataset(val_act, img_vecs_val)\n",
    "    \n",
    "    fc_data.classes = data.classes\n",
    "    fc_data_val.classes = data.classes\n",
    "    \n",
    "    fc_db = DataBunch.create(fc_data, fc_data_val, path=PATH, device=torch.device('cuda'))\n",
    "    return fc_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_features(m:Model)->int:\n",
    "    \"Return the number of output features for a `model`.\"\n",
    "    for l in reversed(flatten_model(m)):\n",
    "        if hasattr(l, 'num_features'): \n",
    "            return l.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_db = save_fc1(data, body, learn.path, 'resnet50', TMP_PATH, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4096]) torch.Size([128, 300])\n",
      "torch.Size([72, 4096]) torch.Size([72, 300])\n"
     ]
    }
   ],
   "source": [
    "for x, y in iter(fc_db.valid_dl):\n",
    "        print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train our custom head from our computed activations. Notice that we are training our head **as part of the original custom ConvNet** and not as a separate sequential object. This is important because it means that we can load our pretrained model into our backbone like we did before, train our head only with the precomputed activations and then train the whole network **without having to join our two parts** (they are trained separately but are still connected in our memory). In summary, once we have loaded our pretrained weights on our backbone and trained our head we can directly train our whole network with differential learning rates without having to do any adjustment. Cool huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = learn.model[1][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Dropout(p=0.2)\n",
       "  (4): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (5): ReLU(inplace)\n",
       "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout(p=0.2)\n",
       "  (8): Linear(in_features=512, out_features=300, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_head = Learner(data=fc_db, model=head[2:], opt_fn = partial(AdamW, betas=(0.9,0.99)), loss_fn = cos_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d61f7d3792b4bdfb503a7302d1d7aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=2), HTML(value='0.00% [0/2 00:00<00:00]'))), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_head.lr_find(start_lr=1e-4, end_lr=1e15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception ignored in: <bound method _ConnectionBase.__del__ of <multiprocessing.connection.Connection object at 0x7f62075242e8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception ignored in: <bound method _ConnectionBase.__del__ of <multiprocessing.connection.Connection object at 0x7f62057e79e8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception ignored in: <bound method _ConnectionBase.__del__ of <multiprocessing.connection.Connection object at 0x7f62057e7400>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/home/francisco/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_head.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=10), HTML(value='0.00% [0/10 00:00<00:00]'))), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:10\n",
      "epoch  train loss  valid loss\n",
      "0      0.464765    0.720897    (00:01)\n",
      "1      0.287167    0.725685    (00:01)\n",
      "2      0.259825    0.723413    (00:01)\n",
      "3      0.252991    0.725350    (00:01)\n",
      "4      0.249688    0.732577    (00:01)\n",
      "5      0.245789    0.734560    (00:01)\n",
      "6      0.238553    0.739007    (00:01)\n",
      "7      0.227044    0.733116    (00:01)\n",
      "8      0.211339    0.737191    (00:01)\n",
      "9      0.202279    0.739396    (00:01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn_head.fit_one_cycle(10, lr, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_head.save('pre0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLearner(data=<fastai.data.DataBunch object at 0x7f621d4dabe0>, model=Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): AdaptiveConcatPool2d(\n",
      "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
      "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Lambda()\n",
      "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2)\n",
      "    (4): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.2)\n",
      "    (8): Linear(in_features=512, out_features=300, bias=True)\n",
      "  )\n",
      "), opt_fn=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_fn=<function cos_loss at 0x7f6231522ea0>, metrics=[], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('../data/imagenet/test'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>, <class 'fastai.train.BnFreeze'>], callbacks=[], layer_groups=[Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU(inplace)\n",
      "  (11): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (13): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (17): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (19): ReLU(inplace)\n",
      "  (20): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (22): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (23): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (24): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (26): ReLU(inplace)\n",
      "  (27): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (29): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (30): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (31): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (33): ReLU(inplace)\n",
      "  (34): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (36): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (38): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (40): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (42): ReLU(inplace)\n",
      "  (43): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (44): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (45): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (46): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (47): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (49): ReLU(inplace)\n",
      "  (50): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (51): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (52): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (53): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (54): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (55): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (56): ReLU(inplace)\n",
      "), Sequential(\n",
      "  (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU(inplace)\n",
      "  (7): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  (8): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (13): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (14): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (15): ReLU(inplace)\n",
      "  (16): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (20): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (21): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (22): ReLU(inplace)\n",
      "  (23): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (27): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (28): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (29): ReLU(inplace)\n",
      "  (30): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (33): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (34): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (35): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (36): ReLU(inplace)\n",
      "  (37): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (39): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (40): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (41): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (42): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (43): ReLU(inplace)\n",
      "  (44): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (48): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (49): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (50): ReLU(inplace)\n",
      "  (51): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  (52): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (53): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (55): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (56): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (57): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (58): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (59): ReLU(inplace)\n",
      "  (60): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (61): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (62): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (63): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (64): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (65): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (66): ReLU(inplace)\n",
      "), Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=1)\n",
      "  (1): AdaptiveMaxPool2d(output_size=1)\n",
      "  (2): Lambda()\n",
      "  (3): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (4): Dropout(p=0.2)\n",
      "  (5): Linear(in_features=4096, out_features=512, bias=True)\n",
      "  (6): ReLU(inplace)\n",
      "  (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Dropout(p=0.2)\n",
      "  (9): Linear(in_features=512, out_features=300, bias=True)\n",
      ")])\n"
     ]
    }
   ],
   "source": [
    "print(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.array([lr/1000,lr/100,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Check how to correctly train with differential learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lrs, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search imagenet classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns, wvs = list(zip(*syn_wv_1k))\n",
    "wvs = np.array(wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "%time pred_wv = learn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "denorm = md.val_ds.denorm\n",
    "\n",
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "def show_imgs(ims, cols, figsize=None):\n",
    "    fig,axes = plt.subplots(len(ims)//cols, cols, figsize=figsize)\n",
    "    for i,ax in enumerate(axes.flat): show_img(ims[i], ax=ax)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e5f8ae797ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "show_imgs(denorm(md.val_ds[start:start+25][0]), 5, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "def create_index(a):\n",
    "    index = nmslib.init(space='angulardist')\n",
    "    index.addDataPointBatch(a)\n",
    "    index.createIndex()\n",
    "    return index\n",
    "\n",
    "def get_knns(index, vecs):\n",
    "     return zip(*index.knnQueryBatch(vecs, k=10, num_threads=4))\n",
    "\n",
    "def get_knn(index, vec): return index.knnQuery(vec, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_wvs = create_index(wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,dists = get_knns(nn_wvs, pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['spoonbill', 'bustard', 'oystercatcher'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[classids[syns[id]] for id in ids[:3]] for ids in idxs[start:start+10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search all wordnet noun classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syns, all_wvs = list(zip(*syn2wv.items()))\n",
    "all_wvs = np.array(all_wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_allwvs = create_index(all_wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,dists = get_knns(nn_allwvs, pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['spoonbill', 'bustard', 'oystercatcher'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[classids[all_syns[id]] for id in ids[:3]] for ids in idxs[start:start+10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text -> image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predwv = create_index(pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecd = pickle.load(open(TRANS_PATH/'wiki.en.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = en_vecd['boat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (en_vecd['engine'] + en_vecd['boat'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (en_vecd['sail'] + en_vecd['boat'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image->image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'valid/n01440764/ILSVRC2012_val_00007197.JPEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image(PATH/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c9b680a767fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_img' is not defined"
     ]
    }
   ],
   "source": [
    "show_img(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_img = md.val_ds.transform(img)\n",
    "pred = learn.predict_array(t_img[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf37ec4574c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, pred)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[1:4]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
