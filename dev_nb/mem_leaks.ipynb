{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking memory leaks / memory fragmentation\n",
    "\n",
    "This notebook is for finding out where fastai doesn't allocate GPU RAM efficiently. Feel free to add other sections. Currently it only does a basic training loop, with some unfreezing and inference.\n",
    "\n",
    "The detection comes from reading the output of [IPyExperimentsPytorch](https://github.com/stas00/ipyexperiments/) per-cell reports.\n",
    "\n",
    "In particular watch Delta Peak column which may indicate where more GPU RAM was allocated before freeing some, which may lead to smalish holes in allocated GPU RAM which can't be re-used and thus causing fragmentation and leading to less total available GPU RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from ipyexperiments import IPyExperimentsPytorch\n",
    "#! pip install ipyexperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert str(device) == 'cuda:0', f\"we want GPU, got {device}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def alert(string, color='red'):\n",
    "    display(Markdown(f\"<span style='color:{color}'>**{string}**</span>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = consume_cpu(2**14) # about 1GB\n",
    "def consume_gpu(n): return torch.ones((n, n)).cuda()\n",
    "def consume_1gb(): return [consume_gpu(2**14)]\n",
    "def consume_6gb(): return [consume_1gb() for x in range(6) ]\n",
    "\n",
    "def reclaim():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc, threading, torch, time, pynvml\n",
    "from fastai.utils.mem import *\n",
    "from fastai.vision import *\n",
    "\n",
    "if not torch.cuda.is_available(): raise Exception(\"pytorch is required\")\n",
    "\n",
    "def preload_pytorch():\n",
    "    torch.ones((1, 1)).cuda()\n",
    "    \n",
    "def gpu_mem_get_used_no_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    return gpu_mem_get().used\n",
    "\n",
    "def gpu_mem_used_get_fast(gpu_handle):\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    return int(info.used/2**20)\n",
    "\n",
    "def torch_mem_report():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(list(map(lambda x: int(x/2**20), [torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()])))\n",
    "    \n",
    "preload_pytorch()\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "class PeakMemMetric(LearnerCallback):\n",
    "    _order=-20 # Needs to run before the recorder\n",
    "\n",
    "    def peak_monitor_start(self):\n",
    "        self.peak_monitoring = True\n",
    "\n",
    "        # start RAM tracing\n",
    "        tracemalloc.start()\n",
    "\n",
    "        # this thread samples RAM usage as long as the current epoch of the fit loop is running\n",
    "        peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)\n",
    "        peak_monitor_thread.daemon = True\n",
    "        peak_monitor_thread.start()\n",
    "        \n",
    "    def peak_monitor_stop(self):\n",
    "        tracemalloc.stop()\n",
    "        self.peak_monitoring = False\n",
    "        \n",
    "    def peak_monitor_func(self):\n",
    "        self.gpu_mem_used_peak = -1\n",
    "\n",
    "        gpu_id = torch.cuda.current_device()\n",
    "        gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_id)\n",
    "\n",
    "        while True:\n",
    "            gpu_mem_used = gpu_mem_used_get_fast(gpu_handle)\n",
    "            self.gpu_mem_used_peak = max(gpu_mem_used, self.gpu_mem_used_peak)\n",
    "            if not self.peak_monitoring: break\n",
    "            time.sleep(0.001) # 1msec\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.learn.recorder.add_metric_names(['cpu used',  'peak', 'gpu used',  'peak'])\n",
    "                    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.peak_monitor_start()\n",
    "        self.gpu_before = gpu_mem_get_used_no_cache()\n",
    "\n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        cpu_current, cpu_peak =  list(map(lambda x: int(x/2**20), tracemalloc.get_traced_memory()))\n",
    "        gpu_current = gpu_mem_get_used_no_cache() - self.gpu_before\n",
    "        gpu_peak    = self.gpu_mem_used_peak      - self.gpu_before\n",
    "        self.peak_monitor_stop()\n",
    "        # The numbers are deltas in MBs (beginning of the epoch and the end)\n",
    "        self.learn.recorder.add_metrics([cpu_current, cpu_peak, gpu_current, gpu_peak])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = IPyExperimentsPytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "defaults.cmap='binary'\n",
    "bs=512\n",
    "tfms = ([*rand_pad(padding=3, size=28, mode='zeros')], [])\n",
    "#arch=\"resnet34\"\n",
    "arch=\"resnet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = ImageItemList.from_folder(path, convert_mode='L')\n",
    "il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = il.split_by_folder(train='training', valid='testing')\n",
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = sd.label_from_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = src.transform(tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ll.databunch(bs=bs).normalize(imagenet_stats)\n",
    "x,y = data.train_ds[0]\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(models, arch) # models.resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_cnn(data, model, metrics=[accuracy], callback_fns=PeakMemMetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd time to check for leaks\n",
    "learn.lr_find()\n",
    "# gpu delta consumed should be zero\n",
    "# but why peaked is much smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reclaim() # resets lr_find's GPU RAM consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, max_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle this:\n",
    "learn.fit_one_cycle(1, max_lr=1e-2)\n",
    "learn.fit_one_cycle(2, max_lr=1e-2)\n",
    "learn.save(f'reload1')\n",
    "_=learn.load(f'reload1')\n",
    "reclaim() # resets lr_find's GPU RAM consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reclaim() # resets fit_one_cycle's GPU RAM consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'reload1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Change\n",
    "end of `learn.load()` is a place where reclaim should be executed, so that the GPU RAM taken previously by the model is unloaded - otherwise it's doubled in size until gc.collect() arrives some time in the future.\n",
    "\n",
    "Currently, we get delta peaked reported @ 126MB for models.resnet34, and it should be 0 peaked, if first the model is unloaded and then loaded again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=learn.load(f'reload1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_mem, gpu_mem, time_data = exp1.cl.data\n",
    "if b2mb(gpu_mem.peaked_delta) > 10:\n",
    "    alert(f\"load() caused potential fragmentation by not unloading model first, delta peaked at {b2mb(gpu_mem.peaked_delta)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reclaim() # resets load's GPU RAM consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reclaim_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(end_lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, max_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'leak2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=learn.load(f'leak2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd time to check for leaks\n",
    "learn.lr_find()\n",
    "# gpu delta consumed should be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "learn.fit_one_cycle(1, slice(1e-5, lr/5))\n",
    "# XXX: something is wrong here measurement-wise\n",
    "# callback reports peak of 942 vs. cell peak reports at 432 - the measuring thread of the cell probably missed that higher peak - need to switch to the new pytorch max_memory_allocated with reset_ to get the exact measurement. must wait for pytorch-1.0.1 to be out.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'leak3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference via learn.export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(path, test=ImageItemList.from_folder(path/'testing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.test_ds\n",
    "len(learn.data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = learn.get_preds(ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions as numpy\n",
    "pred = predictions[0].numpy()\n",
    "pred[0]\n",
    "pred_df = pd.DataFrame(pred)\n",
    "pred_df[:5]\n",
    "#pred_df.sort_values(by=\"preds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with manual learn re-construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn\n",
    "gc.collect()\n",
    "#learn.opt.clear()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "tfms = get_transforms(do_flip=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.add_test_folder(test_folder='testing')\n",
    "        .transform(tfms) # .transform(tfms, size=256)\n",
    "        .databunch().normalize(imagenet_stats))\n",
    "learn = create_cnn(data, model)\n",
    "_=learn.load(f'leak3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.data = (src.add_test_folder(test_folder='test')\n",
    "#         .transform(tfms) # .transform(tfms, size=256)\n",
    "#         .databunch().normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.test_ds\n",
    "len(learn.data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = learn.get_preds(ds_type=DatasetType.Test)\n",
    "\n",
    "# need to also try learn.TTA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions as numpy\n",
    "pred = predictions[0].numpy()\n",
    "pred[0]\n",
    "pred_df = pd.DataFrame(pred)\n",
    "pred_df[:5]\n",
    "#pred_df.sort_values(by=\"preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
